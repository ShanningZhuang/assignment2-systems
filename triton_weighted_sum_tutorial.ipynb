{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Triton Tutorial: Weighted Sum Kernel\n",
        "\n",
        "This notebook walks through implementing a custom weighted sum operation in Triton, based on the CS336 Assignment 2 example.\n",
        "\n",
        "## What we'll learn:\n",
        "1. How to write a Triton kernel with the `@triton.jit` decorator\n",
        "2. How to use block pointers for memory access\n",
        "3. How to implement forward and backward passes\n",
        "4. How to integrate Triton kernels with PyTorch's autograd\n",
        "\n",
        "## The Operation\n",
        "\n",
        "Given:\n",
        "- Input matrix `X` with shape `[..., D]` (can be batched)\n",
        "- Weight vector `w` with shape `[D]`\n",
        "\n",
        "Compute: `(w * X).sum(axis=-1)` - a weighted sum along the last dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Triton version: 3.2.0\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "# Helper function for ceiling division\n",
        "def cdiv(a, b):\n",
        "    return (a + b - 1) // b\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Triton version: {triton.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: PyTorch Reference Implementation\n",
        "\n",
        "First, let's see what we're trying to implement in pure PyTorch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 8])\n",
            "Weight shape: torch.Size([8])\n",
            "Output shape: torch.Size([4])\n",
            "Output: tensor([1.3916, 0.9102, 0.8584, 1.7041], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def weighted_sum_pytorch(x, weight):\n",
        "    \"\"\"Reference implementation in PyTorch\"\"\"\n",
        "    # x has shape [..., D], weight has shape [D]\n",
        "    return (weight * x).sum(axis=-1)\n",
        "\n",
        "# Test it\n",
        "D = 8\n",
        "n_rows = 4\n",
        "x = torch.randn(n_rows, D, device='cuda')\n",
        "weight = torch.randn(D, device='cuda')\n",
        "\n",
        "result = weighted_sum_pytorch(x, weight)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Weight shape: {weight.shape}\")\n",
        "print(f\"Output shape: {result.shape}\")\n",
        "print(f\"Output: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Forward kernel defined!\n"
          ]
        }
      ],
      "source": [
        "@triton.jit\n",
        "def weighted_sum_fwd(\n",
        "    x_ptr, weight_ptr,  # Input pointers\n",
        "    output_ptr,  # Output pointer\n",
        "    x_stride_row, x_stride_dim,  # Strides for x\n",
        "    weight_stride_dim,  # Stride for weight\n",
        "    output_stride_row,  # Stride for output\n",
        "    ROWS, D,  # Dimensions\n",
        "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile sizes (compile-time constants)\n",
        "):\n",
        "    \"\"\"\n",
        "    Triton kernel for weighted sum forward pass.\n",
        "    \n",
        "    Each thread block processes a tile of rows:\n",
        "    - row_tile_idx determines which tile of rows this block handles\n",
        "    - We loop over D in tiles, accumulating the weighted sum\n",
        "    \"\"\"\n",
        "    # Each thread block processes a tile of rows\n",
        "    row_tile_idx = tl.program_id(0)\n",
        "    \n",
        "    # Create block pointer for x (2D: rows × D)\n",
        "    x_block_ptr = tl.make_block_ptr(\n",
        "        x_ptr,\n",
        "        shape=(ROWS, D),\n",
        "        strides=(x_stride_row, x_stride_dim),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),  # Start at our tile\n",
        "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "        order=(1, 0),  # Column-major within block\n",
        "    )\n",
        "    \n",
        "    # Create block pointer for weight (1D: D)\n",
        "    weight_block_ptr = tl.make_block_ptr(\n",
        "        weight_ptr,\n",
        "        shape=(D,),\n",
        "        strides=(weight_stride_dim,),\n",
        "        offsets=(0,),\n",
        "        block_shape=(D_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Create block pointer for output (1D: rows)\n",
        "    output_block_ptr = tl.make_block_ptr(\n",
        "        output_ptr,\n",
        "        shape=(ROWS,),\n",
        "        strides=(output_stride_row,),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
        "        block_shape=(ROWS_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Load first tile to determine dtype, then initialize accumulator\n",
        "    x_tile_first = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "    weight_tile_first = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "    \n",
        "    # Compute first partial sum\n",
        "    weighted_first = x_tile_first * weight_tile_first[None, :]\n",
        "    acc = tl.sum(weighted_first, axis=1)\n",
        "    \n",
        "    # Advance pointers for next iteration\n",
        "    x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))\n",
        "    weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n",
        "    \n",
        "    # Loop over remaining tiles in the D dimension\n",
        "    for i in range(1, tl.cdiv(D, D_TILE_SIZE)):\n",
        "        # Load tiles\n",
        "        x_tile = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "        weight_tile = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "        \n",
        "        # Compute weighted sum for this tile\n",
        "        # x_tile: (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
        "        # weight_tile: (D_TILE_SIZE,)\n",
        "        weighted = x_tile * weight_tile[None, :]  # Broadcast weight\n",
        "        acc += tl.sum(weighted, axis=1)  # Sum along D dimension\n",
        "        \n",
        "        # Advance pointers to next tile\n",
        "        x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))\n",
        "        weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n",
        "    \n",
        "    # Store result\n",
        "    tl.store(output_block_ptr, acc, boundary_check=(0,))\n",
        "\n",
        "print(\"✓ Forward kernel defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Backward kernel defined!\n"
          ]
        }
      ],
      "source": [
        "@triton.jit\n",
        "def weighted_sum_backward(\n",
        "    x_ptr, weight_ptr,  # Inputs from forward pass\n",
        "    grad_output_ptr,  # Gradient w.r.t. output\n",
        "    grad_x_ptr, partial_grad_weight_ptr,  # Gradient outputs\n",
        "    stride_xr, stride_xd,\n",
        "    stride_wd,\n",
        "    stride_gr,\n",
        "    stride_gxr, stride_gxd,\n",
        "    stride_gwb, stride_gwd,\n",
        "    NUM_ROWS, D,\n",
        "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Triton kernel for weighted sum backward pass.\n",
        "    \n",
        "    Computes:\n",
        "    - grad_x[i,j] = weight[j] * grad_output[i] (outer product)\n",
        "    - grad_weight[j] = sum_i(x[i,j] * grad_output[i]) (reduction)\n",
        "    \n",
        "    For grad_weight, we compute partial sums per tile and reduce later.\n",
        "    \"\"\"\n",
        "    row_tile_idx = tl.program_id(0)\n",
        "    n_row_tiles = tl.num_programs(0)\n",
        "    \n",
        "    # Block pointer for grad_output (1D)\n",
        "    grad_output_block_ptr = tl.make_block_ptr(\n",
        "        grad_output_ptr,\n",
        "        shape=(NUM_ROWS,),\n",
        "        strides=(stride_gr,),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
        "        block_shape=(ROWS_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for x (2D)\n",
        "    x_block_ptr = tl.make_block_ptr(\n",
        "        x_ptr,\n",
        "        shape=(NUM_ROWS, D),\n",
        "        strides=(stride_xr, stride_xd),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
        "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for weight (1D)\n",
        "    weight_block_ptr = tl.make_block_ptr(\n",
        "        weight_ptr,\n",
        "        shape=(D,),\n",
        "        strides=(stride_wd,),\n",
        "        offsets=(0,),\n",
        "        block_shape=(D_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for grad_x (2D)\n",
        "    grad_x_block_ptr = tl.make_block_ptr(\n",
        "        grad_x_ptr,\n",
        "        shape=(NUM_ROWS, D),\n",
        "        strides=(stride_gxr, stride_gxd),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
        "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for partial grad_weight (2D: n_tiles × D)\n",
        "    partial_grad_weight_block_ptr = tl.make_block_ptr(\n",
        "        partial_grad_weight_ptr,\n",
        "        shape=(n_row_tiles, D),\n",
        "        strides=(stride_gwb, stride_gwd),\n",
        "        offsets=(row_tile_idx, 0),\n",
        "        block_shape=(1, D_TILE_SIZE),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    \n",
        "    # Load grad_output once (same for all D tiles)\n",
        "    grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "    \n",
        "    # Loop over D dimension\n",
        "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
        "        # Compute grad_x: outer product of grad_output and weight\n",
        "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "        grad_x_tile = grad_output[:, None] * weight[None, :]  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
        "        tl.store(grad_x_block_ptr, grad_x_tile, boundary_check=(0, 1))\n",
        "        \n",
        "        # Compute partial grad_weight: reduce over rows in this tile\n",
        "        x_tile = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "        grad_weight_tile = tl.sum(x_tile * grad_output[:, None], axis=0, keep_dims=True)\n",
        "        tl.store(partial_grad_weight_block_ptr, grad_weight_tile, boundary_check=(1,))\n",
        "        \n",
        "        # Advance pointers\n",
        "        x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))\n",
        "        weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n",
        "        grad_x_block_ptr = tl.advance(grad_x_block_ptr, (0, D_TILE_SIZE))\n",
        "        partial_grad_weight_block_ptr = tl.advance(partial_grad_weight_block_ptr, (0, D_TILE_SIZE))\n",
        "\n",
        "print(\"✓ Backward kernel defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Autograd function created!\n"
          ]
        }
      ],
      "source": [
        "class WeightedSumFunc(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight):\n",
        "        # Save dimensions\n",
        "        D = x.shape[-1]\n",
        "        output_dims = x.shape[:-1]\n",
        "        \n",
        "        # Reshape to 2D for kernel\n",
        "        input_shape = x.shape\n",
        "        x = x.reshape(-1, D)\n",
        "        \n",
        "        # Validation\n",
        "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
        "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
        "        assert x.is_contiguous(), \"x must be contiguous\"\n",
        "        \n",
        "        # Save for backward\n",
        "        ctx.save_for_backward(x, weight)\n",
        "        \n",
        "        # Choose tile sizes\n",
        "        ctx.D_TILE_SIZE = min(triton.next_power_of_2(D) // 16, 128)\n",
        "        ctx.D_TILE_SIZE = max(ctx.D_TILE_SIZE, 1)\n",
        "        ctx.ROWS_TILE_SIZE = 16\n",
        "        ctx.input_shape = input_shape\n",
        "        \n",
        "        # Allocate output\n",
        "        n_rows = x.shape[0]\n",
        "        y = torch.empty(n_rows, device=x.device, dtype=x.dtype)\n",
        "        \n",
        "        # Launch kernel\n",
        "        grid = (cdiv(n_rows, ctx.ROWS_TILE_SIZE),)\n",
        "        weighted_sum_fwd[grid](\n",
        "            x, weight,\n",
        "            y,\n",
        "            x.stride(0), x.stride(1),\n",
        "            weight.stride(0),\n",
        "            y.stride(0),\n",
        "            ROWS=n_rows, D=D,\n",
        "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE,\n",
        "            D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
        "        )\n",
        "        \n",
        "        return y.view(input_shape[:-1])\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, weight = ctx.saved_tensors\n",
        "        ROWS_TILE_SIZE, D_TILE_SIZE = ctx.ROWS_TILE_SIZE, ctx.D_TILE_SIZE\n",
        "        n_rows, D = x.shape\n",
        "        \n",
        "        # Flatten grad_output\n",
        "        grad_output = grad_output.reshape(-1).contiguous()\n",
        "        \n",
        "        # Allocate outputs\n",
        "        grad_x = torch.empty_like(x)\n",
        "        n_tiles = cdiv(n_rows, ROWS_TILE_SIZE)\n",
        "        partial_grad_weight = torch.empty((n_tiles, D), device=x.device, dtype=x.dtype)\n",
        "        \n",
        "        # Launch kernel\n",
        "        grid = (n_tiles,)\n",
        "        weighted_sum_backward[grid](\n",
        "            x, weight,\n",
        "            grad_output,\n",
        "            grad_x, partial_grad_weight,\n",
        "            x.stride(0), x.stride(1),\n",
        "            weight.stride(0),\n",
        "            grad_output.stride(0),\n",
        "            grad_x.stride(0), grad_x.stride(1),\n",
        "            partial_grad_weight.stride(0), partial_grad_weight.stride(1),\n",
        "            NUM_ROWS=n_rows, D=D,\n",
        "            ROWS_TILE_SIZE=ROWS_TILE_SIZE,\n",
        "            D_TILE_SIZE=D_TILE_SIZE,\n",
        "        )\n",
        "        \n",
        "        # Reduce partial gradients\n",
        "        grad_weight = partial_grad_weight.sum(axis=0)\n",
        "        \n",
        "        # Reshape grad_x back to original shape\n",
        "        grad_x = grad_x.view(ctx.input_shape)\n",
        "        \n",
        "        return grad_x, grad_weight\n",
        "\n",
        "# Create the function\n",
        "weighted_sum_triton = WeightedSumFunc.apply\n",
        "\n",
        "print(\"✓ Autograd function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch output shape: torch.Size([32])\n",
            "Triton output shape: torch.Size([32])\n",
            "\n",
            "Max absolute difference: 9.54e-07\n",
            "Mean absolute difference: 3.02e-07\n",
            "\n",
            "✓ Outputs match: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(42)\n",
        "n_rows, D = 32, 64\n",
        "x = torch.randn(n_rows, D, device='cuda', requires_grad=True)\n",
        "weight = torch.randn(D, device='cuda', requires_grad=True)\n",
        "\n",
        "# PyTorch reference\n",
        "output_pytorch = weighted_sum_pytorch(x, weight)\n",
        "\n",
        "# Triton implementation\n",
        "output_triton = weighted_sum_triton(x, weight)\n",
        "\n",
        "# Compare\n",
        "print(f\"PyTorch output shape: {output_pytorch.shape}\")\n",
        "print(f\"Triton output shape: {output_triton.shape}\")\n",
        "print(f\"\\nMax absolute difference: {(output_pytorch - output_triton).abs().max().item():.2e}\")\n",
        "print(f\"Mean absolute difference: {(output_pytorch - output_triton).abs().mean().item():.2e}\")\n",
        "print(f\"\\n✓ Outputs match: {torch.allclose(output_pytorch, output_triton, rtol=1e-4, atol=1e-4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient w.r.t. x:\n",
            "  Max absolute difference: 0.00e+00\n",
            "  ✓ Gradients match: True\n",
            "\n",
            "Gradient w.r.t. weight:\n",
            "  Max absolute difference: 3.55e-15\n",
            "  ✓ Gradients match: True\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "n_rows, D = 32, 64\n",
        "x_pt = torch.randn(n_rows, D, device='cuda', requires_grad=True, dtype=torch.float64)\n",
        "weight_pt = torch.randn(D, device='cuda', requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "x_tr = x_pt.clone().detach().requires_grad_(True)\n",
        "weight_tr = weight_pt.clone().detach().requires_grad_(True)\n",
        "\n",
        "# Forward pass\n",
        "output_pt = weighted_sum_pytorch(x_pt, weight_pt)\n",
        "output_tr = weighted_sum_triton(x_tr, weight_tr)\n",
        "\n",
        "# Backward pass\n",
        "grad_output = torch.randn_like(output_pt)\n",
        "output_pt.backward(grad_output)\n",
        "output_tr.backward(grad_output)\n",
        "\n",
        "# Compare gradients\n",
        "print(\"Gradient w.r.t. x:\")\n",
        "print(f\"  Max absolute difference: {(x_pt.grad - x_tr.grad).abs().max().item():.2e}\")\n",
        "print(f\"  ✓ Gradients match: {torch.allclose(x_pt.grad, x_tr.grad, rtol=1e-4, atol=1e-4)}\")\n",
        "\n",
        "print(\"\\nGradient w.r.t. weight:\")\n",
        "print(f\"  Max absolute difference: {(weight_pt.grad - weight_tr.grad).abs().max().item():.2e}\")\n",
        "print(f\"  ✓ Gradients match: {torch.allclose(weight_pt.grad, weight_tr.grad, rtol=1e-4, atol=1e-4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Shape (16, 32): max diff = 4.77e-07\n",
            "✓ Shape (128, 256): max diff = 5.72e-06\n",
            "✓ Shape (1024, 512): max diff = 1.14e-05\n",
            "✓ Shape (8, 16, 64): max diff = 2.86e-06\n"
          ]
        }
      ],
      "source": [
        "test_shapes = [\n",
        "    (16, 32),      # Small\n",
        "    (128, 256),    # Medium\n",
        "    (1024, 512),   # Large\n",
        "    (8, 16, 64),   # Batched (batch_size=8, seq_len=16, D=64)\n",
        "]\n",
        "\n",
        "for shape in test_shapes:\n",
        "    D = shape[-1]\n",
        "    x = torch.randn(*shape, device='cuda')\n",
        "    weight = torch.randn(D, device='cuda')\n",
        "    \n",
        "    output_pt = weighted_sum_pytorch(x, weight)\n",
        "    output_tr = weighted_sum_triton(x, weight)\n",
        "    \n",
        "    matches = torch.allclose(output_pt, output_tr, rtol=1e-4, atol=1e-4)\n",
        "    max_diff = (output_pt - output_tr).abs().max().item()\n",
        "    \n",
        "    status = \"✓\" if matches else \"✗\"\n",
        "    print(f\"{status} Shape {shape}: max diff = {max_diff:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### 1. Block Pointers\n",
        "`tl.make_block_ptr()` simplifies memory access by handling pointer arithmetic automatically. You specify:\n",
        "- Base pointer and tensor shape\n",
        "- Strides for navigation\n",
        "- Offsets for starting position\n",
        "- Block shape for the tile size\n",
        "\n",
        "### 2. Tiling Strategy\n",
        "We process data in tiles to maximize:\n",
        "- **Memory locality**: Keep data in fast cache/shared memory\n",
        "- **Parallelism**: Each thread block handles a tile independently\n",
        "- **Efficiency**: Reduce memory bandwidth requirements\n",
        "\n",
        "### 3. Program IDs\n",
        "`tl.program_id(0)` lets different thread blocks work on different tiles in parallel. The launch grid `(n_tiles,)` determines how many blocks run.\n",
        "\n",
        "### 4. Boundary Checking\n",
        "`boundary_check` and `padding_option` handle edge cases when tiles don't evenly divide the input dimensions.\n",
        "\n",
        "### 5. Autograd Integration\n",
        "`torch.autograd.Function` connects Triton kernels to PyTorch's autograd system:\n",
        "- `forward()`: Compute output, save tensors for backward\n",
        "- `backward()`: Compute gradients w.r.t. inputs\n",
        "\n",
        "### 6. Backward Pass Strategy\n",
        "For reductions (like `grad_weight`), we:\n",
        "1. Compute partial results per tile\n",
        "2. Store in a buffer\n",
        "3. Reduce outside the kernel with PyTorch\n",
        "\n",
        "## Experiment Further!\n",
        "\n",
        "Try modifying:\n",
        "- **Tile sizes**: Change `ROWS_TILE_SIZE` and `D_TILE_SIZE` to see performance impact\n",
        "- **Input shapes**: Test with different batch sizes and dimensions\n",
        "- **Data types**: Try `float16`, `bfloat16`, or `float64`\n",
        "- **The operation**: Add a bias term, use different aggregation functions\n",
        "- **Optimization**: Add more sophisticated tiling strategies\n",
        "\n",
        "This pattern extends to more complex kernels like:\n",
        "- Attention mechanisms (FlashAttention)\n",
        "- Layer normalization\n",
        "- Custom activations\n",
        "- Matrix multiplications with custom patterns\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "triton",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
