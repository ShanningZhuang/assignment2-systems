{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Triton Tutorial: Weighted Sum Kernel\n",
        "\n",
        "This notebook walks through implementing a custom weighted sum operation in Triton, based on the CS336 Assignment 2 example.\n",
        "\n",
        "## What we'll learn:\n",
        "1. How to write a Triton kernel with the `@triton.jit` decorator\n",
        "2. How to use block pointers for memory access\n",
        "3. How to implement forward and backward passes\n",
        "4. How to integrate Triton kernels with PyTorch's autograd\n",
        "\n",
        "## The Operation\n",
        "\n",
        "Given:\n",
        "- Input matrix `X` with shape `[..., D]` (can be batched)\n",
        "- Weight vector `w` with shape `[D]`\n",
        "\n",
        "Compute: `(w * X).sum(axis=-1)` - a weighted sum along the last dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Triton version: 3.2.0\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "# Helper function for ceiling division\n",
        "def cdiv(a, b):\n",
        "    return (a + b - 1) // b\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Triton version: {triton.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: PyTorch Reference Implementation\n",
        "\n",
        "First, let's see what we're trying to implement in pure PyTorch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 8])\n",
            "Weight shape: torch.Size([8])\n",
            "Output shape: torch.Size([4])\n",
            "Output: tensor([-3.2948, -0.0807,  0.2607, -3.1335], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def weighted_sum_pytorch(x, weight):\n",
        "    \"\"\"Reference implementation in PyTorch\"\"\"\n",
        "    # x has shape [..., D], weight has shape [D]\n",
        "    return (weight * x).sum(axis=-1)\n",
        "\n",
        "# Test it\n",
        "D = 8\n",
        "n_rows = 4\n",
        "x = torch.randn(n_rows, D, device='cuda')\n",
        "weight = torch.randn(D, device='cuda')\n",
        "\n",
        "result = weighted_sum_pytorch(x, weight)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Weight shape: {weight.shape}\")\n",
        "print(f\"Output shape: {result.shape}\")\n",
        "print(f\"Output: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Forward kernel defined!\n"
          ]
        }
      ],
      "source": [
        "@triton.jit\n",
        "def weighted_sum_fwd(\n",
        "    x_ptr, weight_ptr,  # Input pointers\n",
        "    output_ptr,  # Output pointer\n",
        "    x_stride_row, x_stride_dim,  # Strides for x\n",
        "    weight_stride_dim,  # Stride for weight\n",
        "    output_stride_row,  # Stride for output\n",
        "    ROWS, D,  # Dimensions\n",
        "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile sizes (compile-time constants)\n",
        "):\n",
        "    \"\"\"\n",
        "    Triton kernel for weighted sum forward pass.\n",
        "    \n",
        "    Each thread block processes a tile of rows:\n",
        "    - row_tile_idx determines which tile of rows this block handles\n",
        "    - We loop over D in tiles, accumulating the weighted sum\n",
        "    \"\"\"\n",
        "    # Each thread block processes a tile of rows\n",
        "    row_tile_idx = tl.program_id(0)\n",
        "    \n",
        "    # Create block pointer for x (2D: rows × D)\n",
        "    x_block_ptr = tl.make_block_ptr(\n",
        "        x_ptr,\n",
        "        shape=(ROWS, D),\n",
        "        strides=(x_stride_row, x_stride_dim),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),  # Start at our tile\n",
        "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "        order=(1, 0),  # Column-major within block\n",
        "    )\n",
        "    \n",
        "    # Create block pointer for weight (1D: D)\n",
        "    weight_block_ptr = tl.make_block_ptr(\n",
        "        weight_ptr,\n",
        "        shape=(D,),\n",
        "        strides=(weight_stride_dim,),\n",
        "        offsets=(0,),\n",
        "        block_shape=(D_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Create block pointer for output (1D: rows)\n",
        "    output_block_ptr = tl.make_block_ptr(\n",
        "        output_ptr,\n",
        "        shape=(ROWS,),\n",
        "        strides=(output_stride_row,),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
        "        block_shape=(ROWS_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Load first tile to determine dtype, then initialize accumulator\n",
        "    x_tile_first = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "    weight_tile_first = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "    \n",
        "    # Compute first partial sum\n",
        "    weighted_first = x_tile_first * weight_tile_first[None, :]\n",
        "    acc = tl.sum(weighted_first, axis=1)\n",
        "    \n",
        "    # Advance pointers for next iteration\n",
        "    x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))\n",
        "    weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n",
        "    \n",
        "    # Loop over remaining tiles in the D dimension\n",
        "    for i in range(1, tl.cdiv(D, D_TILE_SIZE)):\n",
        "        # Load tiles\n",
        "        x_tile = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "        weight_tile = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "        \n",
        "        # Compute weighted sum for this tile\n",
        "        # x_tile: (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
        "        # weight_tile: (D_TILE_SIZE,)\n",
        "        weighted = x_tile * weight_tile[None, :]  # Broadcast weight\n",
        "        acc += tl.sum(weighted, axis=1)  # Sum along D dimension\n",
        "        \n",
        "        # Advance pointers to next tile\n",
        "        x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))\n",
        "        weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n",
        "    \n",
        "    # Store result\n",
        "    tl.store(output_block_ptr, acc, boundary_check=(0,))\n",
        "\n",
        "print(\"✓ Forward kernel defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Backward kernel defined!\n"
          ]
        }
      ],
      "source": [
        "@triton.jit\n",
        "def weighted_sum_backward(\n",
        "    x_ptr, weight_ptr,  # Inputs from forward pass\n",
        "    grad_output_ptr,  # Gradient w.r.t. output\n",
        "    grad_x_ptr, partial_grad_weight_ptr,  # Gradient outputs\n",
        "    stride_xr, stride_xd,\n",
        "    stride_wd,\n",
        "    stride_gr,\n",
        "    stride_gxr, stride_gxd,\n",
        "    stride_gwb, stride_gwd,\n",
        "    NUM_ROWS, D,\n",
        "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Triton kernel for weighted sum backward pass.\n",
        "    \n",
        "    Computes:\n",
        "    - grad_x[i,j] = weight[j] * grad_output[i] (outer product)\n",
        "    - grad_weight[j] = sum_i(x[i,j] * grad_output[i]) (reduction)\n",
        "    \n",
        "    For grad_weight, we compute partial sums per tile and reduce later.\n",
        "    \"\"\"\n",
        "    row_tile_idx = tl.program_id(0)\n",
        "    n_row_tiles = tl.num_programs(0)\n",
        "    \n",
        "    # Block pointer for grad_output (1D)\n",
        "    grad_output_block_ptr = tl.make_block_ptr(\n",
        "        grad_output_ptr,\n",
        "        shape=(NUM_ROWS,),\n",
        "        strides=(stride_gr,),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
        "        block_shape=(ROWS_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for x (2D)\n",
        "    x_block_ptr = tl.make_block_ptr(\n",
        "        x_ptr,\n",
        "        shape=(NUM_ROWS, D),\n",
        "        strides=(stride_xr, stride_xd),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
        "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for weight (1D)\n",
        "    weight_block_ptr = tl.make_block_ptr(\n",
        "        weight_ptr,\n",
        "        shape=(D,),\n",
        "        strides=(stride_wd,),\n",
        "        offsets=(0,),\n",
        "        block_shape=(D_TILE_SIZE,),\n",
        "        order=(0,),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for grad_x (2D)\n",
        "    grad_x_block_ptr = tl.make_block_ptr(\n",
        "        grad_x_ptr,\n",
        "        shape=(NUM_ROWS, D),\n",
        "        strides=(stride_gxr, stride_gxd),\n",
        "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
        "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    \n",
        "    # Block pointer for partial grad_weight (2D: n_tiles × D)\n",
        "    partial_grad_weight_block_ptr = tl.make_block_ptr(\n",
        "        partial_grad_weight_ptr,\n",
        "        shape=(n_row_tiles, D),\n",
        "        strides=(stride_gwb, stride_gwd),\n",
        "        offsets=(row_tile_idx, 0),\n",
        "        block_shape=(1, D_TILE_SIZE),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    \n",
        "    # Load grad_output once (same for all D tiles)\n",
        "    grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "    \n",
        "    # Loop over D dimension\n",
        "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
        "        # Compute grad_x: outer product of grad_output and weight\n",
        "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
        "        grad_x_tile = grad_output[:, None] * weight[None, :]  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
        "        tl.store(grad_x_block_ptr, grad_x_tile, boundary_check=(0, 1))\n",
        "        \n",
        "        # Compute partial grad_weight: reduce over rows in this tile\n",
        "        x_tile = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "        grad_weight_tile = tl.sum(x_tile * grad_output[:, None], axis=0, keep_dims=True)\n",
        "        tl.store(partial_grad_weight_block_ptr, grad_weight_tile, boundary_check=(1,))\n",
        "        \n",
        "        # Advance pointers\n",
        "        x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))\n",
        "        weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n",
        "        grad_x_block_ptr = tl.advance(grad_x_block_ptr, (0, D_TILE_SIZE))\n",
        "        partial_grad_weight_block_ptr = tl.advance(partial_grad_weight_block_ptr, (0, D_TILE_SIZE))\n",
        "\n",
        "print(\"✓ Backward kernel defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Autograd function created!\n"
          ]
        }
      ],
      "source": [
        "class WeightedSumFunc(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight):\n",
        "        # Save dimensions\n",
        "        D = x.shape[-1]\n",
        "        output_dims = x.shape[:-1]\n",
        "        \n",
        "        # Reshape to 2D for kernel\n",
        "        input_shape = x.shape\n",
        "        x = x.reshape(-1, D)\n",
        "        \n",
        "        # Validation\n",
        "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
        "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
        "        assert x.is_contiguous(), \"x must be contiguous\"\n",
        "        \n",
        "        # Save for backward\n",
        "        ctx.save_for_backward(x, weight)\n",
        "        \n",
        "        # Choose tile sizes\n",
        "        ctx.D_TILE_SIZE = min(triton.next_power_of_2(D) // 16, 128)\n",
        "        ctx.D_TILE_SIZE = max(ctx.D_TILE_SIZE, 1)\n",
        "        ctx.ROWS_TILE_SIZE = 16\n",
        "        ctx.input_shape = input_shape\n",
        "        \n",
        "        # Allocate output\n",
        "        n_rows = x.shape[0]\n",
        "        y = torch.empty(n_rows, device=x.device, dtype=x.dtype)\n",
        "        \n",
        "        # Launch kernel\n",
        "        grid = (cdiv(n_rows, ctx.ROWS_TILE_SIZE),)\n",
        "        weighted_sum_fwd[grid](\n",
        "            x, weight,\n",
        "            y,\n",
        "            x.stride(0), x.stride(1),\n",
        "            weight.stride(0),\n",
        "            y.stride(0),\n",
        "            ROWS=n_rows, D=D,\n",
        "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE,\n",
        "            D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
        "        )\n",
        "        \n",
        "        return y.view(input_shape[:-1])\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, weight = ctx.saved_tensors\n",
        "        ROWS_TILE_SIZE, D_TILE_SIZE = ctx.ROWS_TILE_SIZE, ctx.D_TILE_SIZE\n",
        "        n_rows, D = x.shape\n",
        "        \n",
        "        # Flatten grad_output\n",
        "        grad_output = grad_output.reshape(-1).contiguous()\n",
        "        \n",
        "        # Allocate outputs\n",
        "        grad_x = torch.empty_like(x)\n",
        "        n_tiles = cdiv(n_rows, ROWS_TILE_SIZE)\n",
        "        partial_grad_weight = torch.empty((n_tiles, D), device=x.device, dtype=x.dtype)\n",
        "        \n",
        "        # Launch kernel\n",
        "        grid = (n_tiles,)\n",
        "        weighted_sum_backward[grid](\n",
        "            x, weight,\n",
        "            grad_output,\n",
        "            grad_x, partial_grad_weight,\n",
        "            x.stride(0), x.stride(1),\n",
        "            weight.stride(0),\n",
        "            grad_output.stride(0),\n",
        "            grad_x.stride(0), grad_x.stride(1),\n",
        "            partial_grad_weight.stride(0), partial_grad_weight.stride(1),\n",
        "            NUM_ROWS=n_rows, D=D,\n",
        "            ROWS_TILE_SIZE=ROWS_TILE_SIZE,\n",
        "            D_TILE_SIZE=D_TILE_SIZE,\n",
        "        )\n",
        "        \n",
        "        # Reduce partial gradients\n",
        "        grad_weight = partial_grad_weight.sum(axis=0)\n",
        "        \n",
        "        # Reshape grad_x back to original shape\n",
        "        grad_x = grad_x.view(ctx.input_shape)\n",
        "        \n",
        "        return grad_x, grad_weight\n",
        "\n",
        "# Create the function\n",
        "weighted_sum_triton = WeightedSumFunc.apply\n",
        "\n",
        "print(\"✓ Autograd function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch output shape: torch.Size([32])\n",
            "Triton output shape: torch.Size([32])\n",
            "\n",
            "Max absolute difference: 9.54e-07\n",
            "Mean absolute difference: 3.02e-07\n",
            "\n",
            "✓ Outputs match: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(42)\n",
        "n_rows, D = 32, 64\n",
        "x = torch.randn(n_rows, D, device='cuda', requires_grad=True)\n",
        "weight = torch.randn(D, device='cuda', requires_grad=True)\n",
        "\n",
        "# PyTorch reference\n",
        "output_pytorch = weighted_sum_pytorch(x, weight)\n",
        "\n",
        "# Triton implementation\n",
        "output_triton = weighted_sum_triton(x, weight)\n",
        "\n",
        "# Compare\n",
        "print(f\"PyTorch output shape: {output_pytorch.shape}\")\n",
        "print(f\"Triton output shape: {output_triton.shape}\")\n",
        "print(f\"\\nMax absolute difference: {(output_pytorch - output_triton).abs().max().item():.2e}\")\n",
        "print(f\"Mean absolute difference: {(output_pytorch - output_triton).abs().mean().item():.2e}\")\n",
        "print(f\"\\n✓ Outputs match: {torch.allclose(output_pytorch, output_triton, rtol=1e-4, atol=1e-4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient w.r.t. x:\n",
            "  Max absolute difference: 0.00e+00\n",
            "  ✓ Gradients match: True\n",
            "\n",
            "Gradient w.r.t. weight:\n",
            "  Max absolute difference: 3.55e-15\n",
            "  ✓ Gradients match: True\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "n_rows, D = 32, 64\n",
        "x_pt = torch.randn(n_rows, D, device='cuda', requires_grad=True, dtype=torch.float64)\n",
        "weight_pt = torch.randn(D, device='cuda', requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "x_tr = x_pt.clone().detach().requires_grad_(True)\n",
        "weight_tr = weight_pt.clone().detach().requires_grad_(True)\n",
        "\n",
        "# Forward pass\n",
        "output_pt = weighted_sum_pytorch(x_pt, weight_pt)\n",
        "output_tr = weighted_sum_triton(x_tr, weight_tr)\n",
        "\n",
        "# Backward pass\n",
        "grad_output = torch.randn_like(output_pt)\n",
        "output_pt.backward(grad_output)\n",
        "output_tr.backward(grad_output)\n",
        "\n",
        "# Compare gradients\n",
        "print(\"Gradient w.r.t. x:\")\n",
        "print(f\"  Max absolute difference: {(x_pt.grad - x_tr.grad).abs().max().item():.2e}\")\n",
        "print(f\"  ✓ Gradients match: {torch.allclose(x_pt.grad, x_tr.grad, rtol=1e-4, atol=1e-4)}\")\n",
        "\n",
        "print(\"\\nGradient w.r.t. weight:\")\n",
        "print(f\"  Max absolute difference: {(weight_pt.grad - weight_tr.grad).abs().max().item():.2e}\")\n",
        "print(f\"  ✓ Gradients match: {torch.allclose(weight_pt.grad, weight_tr.grad, rtol=1e-4, atol=1e-4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Shape (16, 32): max diff = 4.77e-07\n",
            "✓ Shape (128, 256): max diff = 5.72e-06\n",
            "✓ Shape (1024, 512): max diff = 1.14e-05\n",
            "✓ Shape (8, 16, 64): max diff = 2.86e-06\n"
          ]
        }
      ],
      "source": [
        "test_shapes = [\n",
        "    (16, 32),      # Small\n",
        "    (128, 256),    # Medium\n",
        "    (1024, 512),   # Large\n",
        "    (8, 16, 64),   # Batched (batch_size=8, seq_len=16, D=64)\n",
        "]\n",
        "\n",
        "for shape in test_shapes:\n",
        "    D = shape[-1]\n",
        "    x = torch.randn(*shape, device='cuda')\n",
        "    weight = torch.randn(D, device='cuda')\n",
        "    \n",
        "    output_pt = weighted_sum_pytorch(x, weight)\n",
        "    output_tr = weighted_sum_triton(x, weight)\n",
        "    \n",
        "    matches = torch.allclose(output_pt, output_tr, rtol=1e-4, atol=1e-4)\n",
        "    max_diff = (output_pt - output_tr).abs().max().item()\n",
        "    \n",
        "    status = \"✓\" if matches else \"✗\"\n",
        "    print(f\"{status} Shape {shape}: max diff = {max_diff:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: PyTorch-Only Custom Autograd Function\n",
        "\n",
        "Now let's implement the same weighted sum operation using **only PyTorch operations** with a custom `autograd.Function`. This demonstrates:\n",
        "\n",
        "1. How to define custom forward and backward passes without Triton\n",
        "2. The mathematical derivations from the assignment (Equation 2)\n",
        "3. How PyTorch's autograd system works under the hood\n",
        "\n",
        "### Mathematical Background\n",
        "\n",
        "Given operation `f(x, w) = (w * x).sum(axis=-1)` where:\n",
        "- `x` has shape `[n, D]` (or `[..., D]` for batched)\n",
        "- `w` has shape `[D]`\n",
        "- Output has shape `[n]` (or `[...]` for batched)\n",
        "\n",
        "**Backward pass gradients** (from Equation 2 in the assignment):\n",
        "\n",
        "1. **Gradient w.r.t. x**: `(∇_x L)_ij = w_j · (∇_f L)_i`\n",
        "   - This is an outer product: `grad_x = grad_output[:, None] * weight[None, :]`\n",
        "\n",
        "2. **Gradient w.r.t. w**: `(∇_w L)_j = Σ_i x_ij · (∇_f L)_i`\n",
        "   - This is a reduction: `grad_weight = (x * grad_output[:, None]).sum(dim=0)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PyTorch custom autograd function created!\n"
          ]
        }
      ],
      "source": [
        "class WeightedSumPyTorchFunc(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom autograd function using pure PyTorch operations.\n",
        "    \n",
        "    This implementation manually defines forward and backward passes,\n",
        "    showing how autograd works without custom kernels.\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight):\n",
        "        \"\"\"\n",
        "        Forward pass: compute (weight * x).sum(axis=-1)\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape [..., D]\n",
        "            weight: Weight vector of shape [D]\n",
        "            \n",
        "        Returns:\n",
        "            output: Tensor of shape [...] (last dimension summed out)\n",
        "        \"\"\"\n",
        "        # Save tensors for backward pass\n",
        "        ctx.save_for_backward(x, weight)\n",
        "        \n",
        "        # Compute weighted sum using PyTorch operations\n",
        "        # Broadcasting: weight[None, :] creates shape [1, D] to broadcast with x\n",
        "        output = (weight * x).sum(dim=-1)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients w.r.t. inputs\n",
        "        \n",
        "        Args:\n",
        "            grad_output: Gradient w.r.t. output, shape [...]\n",
        "            \n",
        "        Returns:\n",
        "            grad_x: Gradient w.r.t. x, shape [..., D]\n",
        "            grad_weight: Gradient w.r.t. weight, shape [D]\n",
        "        \"\"\"\n",
        "        # Retrieve saved tensors\n",
        "        x, weight = ctx.saved_tensors\n",
        "        \n",
        "        # Get the original shape for proper broadcasting\n",
        "        # grad_output has shape [...], we need [..., 1] for broadcasting\n",
        "        grad_output_expanded = grad_output.unsqueeze(-1)  # [..., 1]\n",
        "        \n",
        "        # Gradient w.r.t. x: outer product of grad_output and weight\n",
        "        # Formula: (∇_x L)_ij = w_j · (∇_f L)_i\n",
        "        # grad_output_expanded: [..., 1], weight: [D]\n",
        "        # Result: [..., D]\n",
        "        grad_x = grad_output_expanded * weight\n",
        "        \n",
        "        # Gradient w.r.t. weight: weighted sum of x by grad_output\n",
        "        # Formula: (∇_w L)_j = Σ_i x_ij · (∇_f L)_i\n",
        "        # We need to sum over all dimensions except the last (D)\n",
        "        # x: [..., D], grad_output_expanded: [..., 1]\n",
        "        # First multiply, then sum over all batch dimensions\n",
        "        grad_weight = (x * grad_output_expanded).sum(dim=tuple(range(grad_output.ndim)))\n",
        "        \n",
        "        return grad_x, grad_weight\n",
        "\n",
        "\n",
        "# Create the function\n",
        "weighted_sum_pytorch_custom = WeightedSumPyTorchFunc.apply\n",
        "\n",
        "print(\"✓ PyTorch custom autograd function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Forward Pass Correctness\n",
        "\n",
        "Let's verify that our custom PyTorch autograd function produces the same results as the reference implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference output shape: torch.Size([32])\n",
            "Custom output shape: torch.Size([32])\n",
            "\n",
            "Max absolute difference: 0.00e+00\n",
            "Mean absolute difference: 0.00e+00\n",
            "\n",
            "✓ Outputs match: True\n",
            "\n",
            "Grad function: <torch.autograd.function.WeightedSumPyTorchFuncBackward object at 0x784c1445e470>\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "n_rows, D = 32, 64\n",
        "x = torch.randn(n_rows, D, device='cuda', requires_grad=True)\n",
        "weight = torch.randn(D, device='cuda', requires_grad=True)\n",
        "\n",
        "# Reference implementation\n",
        "output_ref = weighted_sum_pytorch(x, weight)\n",
        "\n",
        "# Custom PyTorch autograd implementation\n",
        "output_custom = weighted_sum_pytorch_custom(x, weight)\n",
        "\n",
        "# Compare\n",
        "print(f\"Reference output shape: {output_ref.shape}\")\n",
        "print(f\"Custom output shape: {output_custom.shape}\")\n",
        "print(f\"\\nMax absolute difference: {(output_ref - output_custom).abs().max().item():.2e}\")\n",
        "print(f\"Mean absolute difference: {(output_ref - output_custom).abs().mean().item():.2e}\")\n",
        "print(f\"\\n✓ Outputs match: {torch.allclose(output_ref, output_custom, rtol=1e-5, atol=1e-8)}\")\n",
        "\n",
        "# Show that it has the correct grad_fn\n",
        "print(f\"\\nGrad function: {output_custom.grad_fn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Backward Pass Correctness\n",
        "\n",
        "Now let's verify that the gradients computed by our custom backward pass match PyTorch's automatic differentiation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient w.r.t. x:\n",
            "  Max absolute difference: 0.00e+00\n",
            "  Mean absolute difference: 0.00e+00\n",
            "  ✓ Gradients match: True\n",
            "\n",
            "Gradient w.r.t. weight:\n",
            "  Max absolute difference: 0.00e+00\n",
            "  Mean absolute difference: 0.00e+00\n",
            "  ✓ Gradients match: True\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "n_rows, D = 32, 64\n",
        "\n",
        "# Create separate tensors for reference and custom implementations\n",
        "x_ref = torch.randn(n_rows, D, device='cuda', requires_grad=True, dtype=torch.float64)\n",
        "weight_ref = torch.randn(D, device='cuda', requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "x_custom = x_ref.clone().detach().requires_grad_(True)\n",
        "weight_custom = weight_ref.clone().detach().requires_grad_(True)\n",
        "\n",
        "# Forward pass\n",
        "output_ref = weighted_sum_pytorch(x_ref, weight_ref)\n",
        "output_custom = weighted_sum_pytorch_custom(x_custom, weight_custom)\n",
        "\n",
        "# Backward pass with same gradient\n",
        "grad_output = torch.randn_like(output_ref)\n",
        "output_ref.backward(grad_output)\n",
        "output_custom.backward(grad_output)\n",
        "\n",
        "# Compare gradients\n",
        "print(\"Gradient w.r.t. x:\")\n",
        "print(f\"  Max absolute difference: {(x_ref.grad - x_custom.grad).abs().max().item():.2e}\")\n",
        "print(f\"  Mean absolute difference: {(x_ref.grad - x_custom.grad).abs().mean().item():.2e}\")\n",
        "print(f\"  ✓ Gradients match: {torch.allclose(x_ref.grad, x_custom.grad, rtol=1e-5, atol=1e-8)}\")\n",
        "\n",
        "print(\"\\nGradient w.r.t. weight:\")\n",
        "print(f\"  Max absolute difference: {(weight_ref.grad - weight_custom.grad).abs().max().item():.2e}\")\n",
        "print(f\"  Mean absolute difference: {(weight_ref.grad - weight_custom.grad).abs().mean().item():.2e}\")\n",
        "print(f\"  ✓ Gradients match: {torch.allclose(weight_ref.grad, weight_custom.grad, rtol=1e-5, atol=1e-8)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Gradient Checking with torch.autograd.gradcheck\n",
        "\n",
        "PyTorch provides `gradcheck` to numerically verify that our backward pass is correct by comparing it to finite differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Gradient check passed: True\n",
            "\n",
            "✅ Our custom backward implementation is mathematically correct!\n",
            "   The gradients match numerical finite difference approximations.\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "# Use small inputs and double precision for numerical stability\n",
        "torch.manual_seed(42)\n",
        "x_test = torch.randn(4, 8, device='cuda', dtype=torch.float64, requires_grad=True)\n",
        "weight_test = torch.randn(8, device='cuda', dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "# gradcheck takes a function and inputs, then numerically checks gradients\n",
        "test_passed = gradcheck(\n",
        "    weighted_sum_pytorch_custom, \n",
        "    (x_test, weight_test),\n",
        "    eps=1e-6,\n",
        "    atol=1e-4,\n",
        "    rtol=1e-3,\n",
        "    raise_exception=False\n",
        ")\n",
        "\n",
        "print(f\"✓ Gradient check passed: {test_passed}\")\n",
        "\n",
        "if test_passed:\n",
        "    print(\"\\n✅ Our custom backward implementation is mathematically correct!\")\n",
        "    print(\"   The gradients match numerical finite difference approximations.\")\n",
        "else:\n",
        "    print(\"\\n❌ Gradient check failed - there may be an error in the backward pass.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 4: Batched Inputs\n",
        "\n",
        "Test with various shapes including batched inputs to ensure our implementation handles arbitrary dimensions correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing custom PyTorch autograd function with various shapes:\n",
            "\n",
            "✓ Shape (16, 32):\n",
            "   Forward max diff: 0.00e+00\n",
            "   Backward (x): True, Backward (w): True\n",
            "✓ Shape (128, 256):\n",
            "   Forward max diff: 0.00e+00\n",
            "   Backward (x): True, Backward (w): True\n",
            "✓ Shape (8, 16, 64):\n",
            "   Forward max diff: 0.00e+00\n",
            "   Backward (x): True, Backward (w): True\n",
            "✓ Shape (4, 8, 16, 32):\n",
            "   Forward max diff: 0.00e+00\n",
            "   Backward (x): True, Backward (w): True\n"
          ]
        }
      ],
      "source": [
        "test_shapes = [\n",
        "    (16, 32),           # 2D: Simple case\n",
        "    (128, 256),         # 2D: Larger\n",
        "    (8, 16, 64),        # 3D: Batched (batch_size=8, seq_len=16, D=64)\n",
        "    (4, 8, 16, 32),     # 4D: Multi-batch (batch=4, heads=8, seq=16, D=32)\n",
        "]\n",
        "\n",
        "print(\"Testing custom PyTorch autograd function with various shapes:\\n\")\n",
        "\n",
        "for shape in test_shapes:\n",
        "    D = shape[-1]\n",
        "    x = torch.randn(*shape, device='cuda', requires_grad=True)\n",
        "    weight = torch.randn(D, device='cuda', requires_grad=True)\n",
        "    \n",
        "    # Forward pass\n",
        "    output_ref = weighted_sum_pytorch(x, weight)\n",
        "    output_custom = weighted_sum_pytorch_custom(x, weight)\n",
        "    \n",
        "    # Check forward pass\n",
        "    forward_matches = torch.allclose(output_ref, output_custom, rtol=1e-5, atol=1e-8)\n",
        "    max_diff = (output_ref - output_custom).abs().max().item()\n",
        "    \n",
        "    # Backward pass\n",
        "    grad_output = torch.randn_like(output_ref)\n",
        "    \n",
        "    x_ref = x.clone().detach().requires_grad_(True)\n",
        "    weight_ref = weight.clone().detach().requires_grad_(True)\n",
        "    output_ref = weighted_sum_pytorch(x_ref, weight_ref)\n",
        "    output_ref.backward(grad_output)\n",
        "    \n",
        "    x_custom = x.clone().detach().requires_grad_(True)\n",
        "    weight_custom = weight.clone().detach().requires_grad_(True)\n",
        "    output_custom = weighted_sum_pytorch_custom(x_custom, weight_custom)\n",
        "    output_custom.backward(grad_output)\n",
        "    \n",
        "    # Check backward pass\n",
        "    grad_x_matches = torch.allclose(x_ref.grad, x_custom.grad, rtol=1e-5, atol=1e-8)\n",
        "    grad_w_matches = torch.allclose(weight_ref.grad, weight_custom.grad, rtol=1e-5, atol=1e-8)\n",
        "    \n",
        "    status = \"✓\" if (forward_matches and grad_x_matches and grad_w_matches) else \"✗\"\n",
        "    print(f\"{status} Shape {shape}:\")\n",
        "    print(f\"   Forward max diff: {max_diff:.2e}\")\n",
        "    print(f\"   Backward (x): {grad_x_matches}, Backward (w): {grad_w_matches}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 5: Visualizing the Gradient Flow\n",
        "\n",
        "Let's visualize how gradients flow through our custom operation in a simple computational graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input x:\n",
            "tensor([[ 0.1940,  2.1614, -0.1721,  0.8491],\n",
            "        [-1.9244,  0.6530, -0.6494, -0.8175],\n",
            "        [ 0.5280, -1.2753, -1.6621, -0.3033]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "\n",
            "Weight w:\n",
            "tensor([ 0.1391, -0.1082, -0.7174,  0.7566], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "\n",
            "Output (weighted sum):\n",
            "tensor([ 0.5590, -0.4911,  1.1744], device='cuda:0',\n",
            "       grad_fn=<WeightedSumPyTorchFuncBackward>)\n",
            "Output shape: torch.Size([3])\n",
            "\n",
            "Loss (sum of outputs): 1.2423\n",
            "\n",
            "Gradient w.r.t. x:\n",
            "tensor([[ 0.1391, -0.1082, -0.7174,  0.7566],\n",
            "        [ 0.1391, -0.1082, -0.7174,  0.7566],\n",
            "        [ 0.1391, -0.1082, -0.7174,  0.7566]], device='cuda:0')\n",
            "Shape: torch.Size([3, 4])\n",
            "\n",
            "Gradient w.r.t. weight:\n",
            "tensor([-1.2024,  1.5390, -2.4836, -0.2718], device='cuda:0')\n",
            "Shape: torch.Size([4])\n",
            "\n",
            "============================================================\n",
            "Understanding the gradients:\n",
            "============================================================\n",
            "\n",
            "1. grad_x = grad_output[:, None] * weight[None, :]\n",
            "   - Each element x[i,j] contributes to output[i] via weight[j]\n",
            "   - So grad_x[i,j] = grad_output[i] * weight[j]\n",
            "\n",
            "2. grad_weight = (x * grad_output[:, None]).sum(dim=0)\n",
            "   - Each weight[j] affects all outputs through x[:, j]\n",
            "   - So grad_weight[j] = sum_i(x[i,j] * grad_output[i])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a simple example\n",
        "x = torch.randn(3, 4, device='cuda', requires_grad=True)\n",
        "weight = torch.randn(4, device='cuda', requires_grad=True)\n",
        "\n",
        "print(\"Input x:\")\n",
        "print(x)\n",
        "print(f\"\\nWeight w:\")\n",
        "print(weight)\n",
        "\n",
        "# Forward pass\n",
        "output = weighted_sum_pytorch_custom(x, weight)\n",
        "print(f\"\\nOutput (weighted sum):\")\n",
        "print(output)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Create a simple loss (sum of outputs)\n",
        "loss = output.sum()\n",
        "print(f\"\\nLoss (sum of outputs): {loss.item():.4f}\")\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nGradient w.r.t. x:\")\n",
        "print(x.grad)\n",
        "print(f\"Shape: {x.grad.shape}\")\n",
        "\n",
        "print(f\"\\nGradient w.r.t. weight:\")\n",
        "print(weight.grad)\n",
        "print(f\"Shape: {weight.grad.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Understanding the gradients:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. grad_x = grad_output[:, None] * weight[None, :]\")\n",
        "print(\"   - Each element x[i,j] contributes to output[i] via weight[j]\")\n",
        "print(\"   - So grad_x[i,j] = grad_output[i] * weight[j]\")\n",
        "print(\"\\n2. grad_weight = (x * grad_output[:, None]).sum(dim=0)\")\n",
        "print(\"   - Each weight[j] affects all outputs through x[:, j]\")\n",
        "print(\"   - So grad_weight[j] = sum_i(x[i,j] * grad_output[i])\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison: PyTorch Custom vs Triton vs Reference\n",
        "\n",
        "Let's compare all three implementations side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forward Pass Comparison:\n",
            "============================================================\n",
            "Reference output shape: torch.Size([128])\n",
            "Custom PyTorch output shape: torch.Size([128])\n",
            "Triton output shape: torch.Size([128])\n",
            "\n",
            "Custom vs Reference - Max diff: 0.00e+00\n",
            "Triton vs Reference - Max diff: 5.72e-06\n",
            "Custom vs Triton - Max diff: 5.72e-06\n",
            "\n",
            "============================================================\n",
            "Backward Pass Comparison:\n",
            "============================================================\n",
            "\n",
            "Gradient w.r.t. x:\n",
            "  Custom vs Reference - Max diff: 0.00e+00\n",
            "  Triton vs Reference - Max diff: 0.00e+00\n",
            "\n",
            "Gradient w.r.t. weight:\n",
            "  Custom vs Reference - Max diff: 0.00e+00\n",
            "  Triton vs Reference - Max diff: 3.81e-06\n",
            "\n",
            "✅ All implementations produce consistent results!\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "n_rows, D = 128, 256\n",
        "\n",
        "x = torch.randn(n_rows, D, device='cuda', dtype=torch.float32)\n",
        "weight = torch.randn(D, device='cuda', dtype=torch.float32)\n",
        "\n",
        "# Test all three implementations\n",
        "output_ref = weighted_sum_pytorch(x, weight)\n",
        "output_custom = weighted_sum_pytorch_custom(x, weight)\n",
        "output_triton = weighted_sum_triton(x, weight)\n",
        "\n",
        "print(\"Forward Pass Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Reference output shape: {output_ref.shape}\")\n",
        "print(f\"Custom PyTorch output shape: {output_custom.shape}\")\n",
        "print(f\"Triton output shape: {output_triton.shape}\")\n",
        "\n",
        "print(f\"\\nCustom vs Reference - Max diff: {(output_custom - output_ref).abs().max().item():.2e}\")\n",
        "print(f\"Triton vs Reference - Max diff: {(output_triton - output_ref).abs().max().item():.2e}\")\n",
        "print(f\"Custom vs Triton - Max diff: {(output_custom - output_triton).abs().max().item():.2e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Backward Pass Comparison:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test gradients\n",
        "x_ref = x.clone().detach().requires_grad_(True)\n",
        "weight_ref = weight.clone().detach().requires_grad_(True)\n",
        "x_custom = x.clone().detach().requires_grad_(True)\n",
        "weight_custom = weight.clone().detach().requires_grad_(True)\n",
        "x_triton = x.clone().detach().requires_grad_(True)\n",
        "weight_triton = weight.clone().detach().requires_grad_(True)\n",
        "\n",
        "output_ref = weighted_sum_pytorch(x_ref, weight_ref)\n",
        "output_custom = weighted_sum_pytorch_custom(x_custom, weight_custom)\n",
        "output_triton = weighted_sum_triton(x_triton, weight_triton)\n",
        "\n",
        "grad_output = torch.randn_like(output_ref)\n",
        "output_ref.backward(grad_output)\n",
        "output_custom.backward(grad_output)\n",
        "output_triton.backward(grad_output)\n",
        "\n",
        "print(f\"\\nGradient w.r.t. x:\")\n",
        "print(f\"  Custom vs Reference - Max diff: {(x_custom.grad - x_ref.grad).abs().max().item():.2e}\")\n",
        "print(f\"  Triton vs Reference - Max diff: {(x_triton.grad - x_ref.grad).abs().max().item():.2e}\")\n",
        "\n",
        "print(f\"\\nGradient w.r.t. weight:\")\n",
        "print(f\"  Custom vs Reference - Max diff: {(weight_custom.grad - weight_ref.grad).abs().max().item():.2e}\")\n",
        "print(f\"  Triton vs Reference - Max diff: {(weight_triton.grad - weight_ref.grad).abs().max().item():.2e}\")\n",
        "\n",
        "print(\"\\n✅ All implementations produce consistent results!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### PyTorch Custom Autograd Functions\n",
        "\n",
        "**When to use PyTorch-only custom autograd:**\n",
        "- You need custom gradient behavior (e.g., gradient clipping, custom chain rule)\n",
        "- You want to understand how autograd works internally\n",
        "- You're implementing operations that compose well with existing PyTorch ops\n",
        "- You don't need low-level performance optimization\n",
        "\n",
        "**Key concepts:**\n",
        "1. **`torch.autograd.Function`**: Base class for custom differentiable operations\n",
        "2. **`forward(ctx, *args)`**: Compute output, save tensors needed for backward\n",
        "3. **`backward(ctx, grad_output)`**: Compute gradients w.r.t. inputs using chain rule\n",
        "4. **`ctx.save_for_backward()`**: Efficiently save tensors for backward pass\n",
        "5. **Broadcasting**: Use `unsqueeze()` and broadcasting for gradient computation\n",
        "\n",
        "**Mathematical formulas (from assignment Equation 2):**\n",
        "- Forward: `f(x, w) = (w * x).sum(axis=-1)`\n",
        "- Backward for x: `(∇_x L)_ij = w_j · (∇_f L)_i` (outer product)\n",
        "- Backward for w: `(∇_w L)_j = Σ_i x_ij · (∇_f L)_i` (reduction)\n",
        "\n",
        "---\n",
        "\n",
        "### Triton Custom Kernels\n",
        "\n",
        "**When to use Triton:**\n",
        "- You need high performance for custom operations\n",
        "- Memory access patterns matter (e.g., FlashAttention)\n",
        "- You want GPU-level optimization without writing CUDA\n",
        "- Standard PyTorch ops don't fuse well for your use case\n",
        "\n",
        "**Key concepts:**\n",
        "1. **Block Pointers**: `tl.make_block_ptr()` simplifies memory access\n",
        "2. **Tiling Strategy**: Process data in tiles for memory locality and parallelism\n",
        "3. **Program IDs**: `tl.program_id(0)` divides work across thread blocks\n",
        "4. **Boundary Checking**: Handle edge cases when tiles don't evenly divide inputs\n",
        "5. **Manual Memory Management**: Explicit loads/stores with pointer arithmetic\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison Summary\n",
        "\n",
        "| Aspect | PyTorch Custom | Triton Kernel |\n",
        "|--------|---------------|---------------|\n",
        "| **Complexity** | Low - just Python | Medium - need GPU concepts |\n",
        "| **Performance** | Good - optimized PyTorch ops | Excellent - fine-grained control |\n",
        "| **Use Case** | Custom gradients, simple ops | Performance-critical kernels |\n",
        "| **Debugging** | Easy - standard Python | Harder - GPU-specific issues |\n",
        "| **Portability** | Works on CPU/GPU | GPU-only |\n",
        "\n",
        "---\n",
        "\n",
        "### Backward Pass Strategies\n",
        "\n",
        "**PyTorch approach:**\n",
        "- Use `.sum(dim=...)` to reduce over batch dimensions\n",
        "- PyTorch handles the reduction efficiently\n",
        "\n",
        "**Triton approach:**\n",
        "1. Compute partial results per tile\n",
        "2. Store in a buffer\n",
        "3. Reduce outside the kernel with PyTorch\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment Further!\n",
        "\n",
        "Try modifying:\n",
        "- **The operation**: Add a bias term, use different aggregation functions (max, mean)\n",
        "- **Custom gradients**: Implement gradient clipping or custom scaling\n",
        "- **Input shapes**: Test with different batch sizes and dimensions\n",
        "- **Data types**: Try `float16`, `bfloat16`, or `float64`\n",
        "- **Composition**: Chain multiple custom autograd functions together\n",
        "- **Numerical stability**: Add epsilon terms or use stable formulations\n",
        "\n",
        "This pattern extends to more complex operations:\n",
        "- **Attention mechanisms**: Custom attention with different scoring functions\n",
        "- **Layer normalization**: With learnable affine parameters\n",
        "- **Custom activations**: GELU, Swish with custom backward passes\n",
        "- **Sparse operations**: Custom sparse matrix operations\n",
        "- **Quantization**: Custom quantization/dequantization ops\n",
        "\n",
        "---\n",
        "\n",
        "### Further Reading\n",
        "- [PyTorch Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n",
        "- [Extending PyTorch](https://pytorch.org/docs/stable/notes/extending.html)\n",
        "- [Triton Documentation](https://triton-lang.org/)\n",
        "- [CS336 Assignment 2](https://stanford-cs336.github.io/spring2025/assignments/assignment2.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "triton",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
